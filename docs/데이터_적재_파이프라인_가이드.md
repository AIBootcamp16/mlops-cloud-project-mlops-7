# 📊 데이터 적재 파이프라인 가이드

## 📋 개요

이 문서는 기상 데이터(ASOS, PM10) 수집부터 ML 모델 학습용 데이터 준비까지의 전체 데이터 적재 파이프라인을 설명합니다. 실시간 데이터 수집, S3 저장, 피처 엔지니어링, Rolling Window 관리까지 모든 과정이 자동화되어 있습니다.

---

## 🏗️ 시스템 아키텍처

```
┌─────────────────────────────────────────────────────────────────┐
│                      시간별 DAG (매시간 10분)                      │
├─────────────────────────────────────────────────────────────────┤
│ [KMA API] → [데이터 수집] → [S3 저장] → [피처 엔지니어링]        │
│     ↓           ↓              ↓                ↓                │
│  ASOS/PM10   파싱/검증    고정경로 덮어쓰기   34개 피처 생성      │
│                                ↓                ↓                │
│                    ml_dataset/latest.parquet (실시간 추론용)      │
│                    weather_pm10_integrated_full.csv (학습용 누적) │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│                  주간 DAG (일요일 새벽 2시)                        │
├─────────────────────────────────────────────────────────────────┤
│              weather_pm10_integrated_full.csv                   │
│                           ↓                                      │
│              Rolling Window 정리 (630일 cutoff)                  │
│                           ↓                                      │
│           오래된 데이터 삭제 + 중복 제거 + 정렬                    │
└─────────────────────────────────────────────────────────────────┘
```

### 📁 S3 데이터 계층 구조 (고정 경로 방식)

```
s3://weather-mlops-team-data/
├── raw/                              # 원시 API 응답 데이터
│   ├── asos/
│   │   └── latest.txt                # 매시간 덮어쓰기 (고정 경로)
│   └── pm10/
│       └── latest.txt                # 매시간 덮어쓰기 (고정 경로)
├── processed/                        # 파싱된 JSON 데이터
│   ├── asos/
│   │   └── latest.json               # 매시간 덮어쓰기 (고정 경로)
│   └── pm10/
│       └── latest.json               # 매시간 덮어쓰기 (고정 경로)
├── mal_dataset/                      # ML 데이터셋
│   ├── train/                        # 학습용 데이터
│   │   └── latest.parquet            # 매시간 덮어쓰기 (고정 경로, 34 features)
│   └── predict/                      # 예측용 데이터 (모델링팀 요청)
│       └── latest.parquet            # 예측 입력 데이터
└── weather_pm10_integrated_full.csv  # 마스터 데이터셋 (학습용, 21개월 Rolling Window)
```

**📌 고정 경로 방식 장점**:
- ✅ 월/연도 변경 걱정 없음 (항상 같은 경로)
- ✅ API 단순화 (`mal_dataset/train/latest.parquet` 고정 사용)
- ✅ 스토리지 최소화 (파일 6개만 유지)
- ✅ 파일 관리 불필요 (자동 덮어쓰기)
- ✅ 학습/예측 데이터 분리 (train/, predict/ 폴더 구분)

---

## 🔄 데이터 파이프라인 프로세스

### 🕐 **시간별 파이프라인** (매시간 10분)

**DAG**: `weather_data_pipeline.py`
**스케줄**: `'10 * * * *'` (매시 10분)
**역할**: 실시간 데이터 수집 및 학습 데이터 축적

### 1️⃣ **실시간 데이터 수집**

```python
# KMA API에서 데이터 수집
asos_raw = kma_client.fetch_asos(target_time)     # 지상관측 데이터
pm10_raw = kma_client.fetch_pm10(target_time)     # 미세먼지 데이터
```

**📍 참고**: UV 데이터 수집은 프로젝트 범위에서 제외되었습니다.

### 2️⃣ **데이터 파싱 및 검증**

```python
# 원시 데이터 파싱
parsed_asos = parsers.parse_asos_raw(asos_raw)
parsed_pm10 = parsers.parse_pm10_raw(pm10_raw)

# 데이터 구조 예시
{
    "station_id": "100",
    "observed_at": "2024-09-29T15:00:00Z",
    "category": "asos",
    "value": 22.5,
    "unit": "°C"
}
```

### 3️⃣ **S3 고정 경로 저장**

**저장 방식**: 고정 경로 덮어쓰기 (월/일 무관)

```python
# 고정 경로로 항상 같은 파일에 저장
raw_key = f"raw/{data_type}/latest.txt"
processed_key = f"processed/{data_type}/latest.json"
ml_key = "mal_dataset/train/latest.parquet"  # 학습용
predict_key = "mal_dataset/predict/latest.parquet"  # 예측용
```

**🎯 장점**:
- 월/연도 변경 무관 (항상 같은 경로)
- API 단순화 (고정 경로 사용)
- 스토리지 최소화 (파일 5개만 유지)

### 4️⃣ **피처 엔지니어링** (34개 피처 자동 생성)

#### 📊 생성되는 피처

- **ASOS 9개 필드**: 온도, 풍속, 습도, 기압, 강수량, 풍향, 이슬점, 운량, 가시거리
- **PM10 데이터**: 미세먼지 농도
- **파생 피처**: 시간대, 계절, 지역 정보 등
- **총 34개 피처** 자동 생성

### 5️⃣ **마스터 CSV 축적**

```python
# 매시간 마스터 CSV에 append (Rolling Window 없이)
existing_df = weather_handler.load_csv_from_s3('weather_pm10_integrated_full.csv')
updated_df = pd.concat([existing_df, new_data_df], ignore_index=True)
weather_handler.save_csv_to_s3(updated_df, 'weather_pm10_integrated_full.csv')

# 📝 Note: Rolling Window 정리는 주간 DAG에서 실행
```

---

### 📅 **주간 파이프라인** (일요일 새벽 2시)

**DAG**: `master_data_update_dag.py`
**스케줄**: `'0 2 * * 0'` (일요일 새벽 2시)
**역할**: Rolling Window 정리 (21개월 = 630일)

### 1️⃣ **Rolling Window 적용**

```python
# 마스터 CSV 로드
existing_df = weather_handler.load_csv_from_s3('weather_pm10_integrated_full.csv')

# 630일 이전 데이터 제거
retention_days = 630
cutoff_date = datetime.now() - timedelta(days=retention_days)
cleaned_df = existing_df[existing_df['datetime'] >= cutoff_date]

# 중복 제거 (최신 데이터 우선)
cleaned_df = cleaned_df.drop_duplicates(subset=['datetime', 'STN'], keep='last')

# 정렬
cleaned_df = cleaned_df.sort_values(['datetime', 'STN'])

# 저장
weather_handler.save_csv_to_s3(cleaned_df, 'weather_pm10_integrated_full.csv')
```

### 2️⃣ **데이터 품질 검증**

```python
# 날짜 범위 확인
date_min = cleaned_df['datetime'].min()
date_max = cleaned_df['datetime'].max()

# 중복 체크
duplicate_count = cleaned_df.duplicated(subset=['datetime', 'STN']).sum()
# → 0이어야 함

# 레코드 수 확인
total_records = len(cleaned_df)
# → 약 15만개 (21개월치)
```

---

## 🔧 핵심 컴포넌트

### 📡 **KMAApiClient** (`src/data/kma_client.py`)

```python
class KMAApiClient:
    def fetch_asos(self, target_time) -> str        # 지상관측 데이터
    def fetch_pm10(self, start_time, end_time) -> str  # 미세먼지 데이터
    # fetch_uv() 메서드는 제거됨
```

### 🏭 **WeatherDataProcessor** (`src/data/weather_processor.py`)

```python
class WeatherDataProcessor:
    def process_and_store_weather_data()           # 실시간 데이터 처리
    def update_master_training_dataset()          # Rolling Window 마스터 업데이트
    def load_latest_ml_dataset()                  # 최신 ML 데이터셋 로드
```

### 🗄️ **WeatherDataS3Handler** (`src/storage/s3_client.py`)

```python
class WeatherDataS3Handler:
    def save_raw_weather_data()                   # 원시 데이터 저장 (고정 경로 덮어쓰기)
    def save_parsed_weather_data()                # 파싱 데이터 저장 (고정 경로 덮어쓰기)
    def save_ml_dataset()                         # ML 학습 데이터셋 저장 (train/)
    def save_predict_dataset()                    # 예측용 데이터셋 저장 (predict/)
    def load_latest_ml_dataset()                  # 최신 ML 데이터셋 로드 (train/)
    def load_predict_dataset()                    # 예측용 데이터셋 로드 (predict/)
    def save_csv_to_s3()                          # 마스터 CSV 저장
    def load_csv_from_s3()                        # 마스터 CSV 로드
```

### ⚙️ **Feature Builder** (`src/features/feature_builder.py`)

```python
def create_ml_dataset(raw_data, include_labels=False):
    # include_labels=False → 추론용 (comfort_score 제외)
    # include_labels=True  → 학습용 (comfort_score 포함)
```

---

## 🚀 실행 방법

### 1️⃣ **수동 실행** (개발/테스트용)

```bash
cd /path/to/mlops-cloud-project
python src/data/weather_processor.py
```

### 2️⃣ **Airflow 자동화** (운영환경)

#### 시간별 DAG
```bash
# DAG 파일: Airflow/dag/weather_data_pipeline.py
# 스케줄: 매시간 10분 (10 * * * *)
# 예: 13:10, 14:10, 15:10, ...
# 역할: 데이터 수집 + 실시간 parquet 생성 + CSV 축적
```

#### 주간 DAG
```bash
# DAG 파일: Airflow/dag/master_data_update_dag.py
# 스케줄: 일요일 새벽 2시 (0 2 * * 0)
# 역할: Rolling Window 정리 (630일 이전 데이터 삭제)
```

### 3️⃣ **환경 변수 설정**

```bash
# KMA API 설정
export KMA_API_KEY="your_api_key"
export KMA_BASE_URL="https://apis.data.go.kr/1360000/AsosHourlyInfoService"

# S3 설정
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_REGION="ap-northeast-2"
export S3_BUCKET_NAME="mlops-weather-bucket"
```

---

## 📊 데이터 품질 관리

### 🔍 **데이터 검증**

- **형식 검증**: JSON 스키마 검증, 필수 필드 확인
- **값 검증**: 온도 범위(-50~50°C), PM10 범위(0~999)
- **중복 제거**: 동일 시간/관측소 데이터 최신 우선
- **누락 처리**: 결측값 탐지 및 보간

### 📈 **모니터링 지표**

```python
# 데이터 인벤토리 확인
inventory = weather_handler.get_data_inventory()
{
    "raw_data": 1440,          # 일일 데이터 수
    "processed_data": 1440,    # 처리된 데이터 수
    "ml_datasets": 720,        # ML 데이터셋 수
    "master_data": 1,          # 마스터 파일 수
    "total": 3601              # 전체 객체 수
}
```

---

## 🎯 ML 모델 활용

### 🔮 **실시간 추론 파이프라인**

```python
# 1. 최신 실시간 데이터 가져오기
latest_data = processor.load_latest_ml_dataset(days_back=1)

# 2. 모델 추론 (comfort_score 예측)
prediction = model.predict(latest_data.drop(['comfort_score'], axis=1))

# 3. 예측 결과 저장/서빙
```

### 🎓 **모델 재훈련 파이프라인**

```python
# 1. 마스터 학습 데이터 로드 (21개월)
training_data = processor.load_csv_from_s3('weather_pm10_integrated_full.csv')

# 2. 학습용 피처 생성 (comfort_score 포함)
ml_features = create_ml_dataset(training_data, include_labels=True)

# 3. 모델 재훈련 및 성능 평가
model.fit(X_train, y_train)
```

---

## 🔧 트러블슈팅

### ❌ **자주 발생하는 문제들**

#### 1️⃣ **KMA API 연결 실패**
```bash
# 해결: API 키 및 네트워크 확인
export KMA_API_KEY="올바른_API_키"
curl -I https://apis.data.go.kr/1360000/AsosHourlyInfoService
```

#### 2️⃣ **S3 권한 오류**
```bash
# 해결: IAM 권한 확인
aws s3 ls s3://mlops-weather-bucket/
```

#### 3️⃣ **파일 로드 실패**
```python
# 해결: 고정 경로 확인
# ml_dataset/latest.parquet 파일 존재 여부 확인
weather_handler.load_latest_ml_dataset()
```

#### 4️⃣ **Rolling Window 데이터 부족**
```python
# 해결: retention_days 조정
processor.update_master_training_dataset(
    new_data, retention_days=365  # 12개월로 단축
)
```

---

## 📚 참고 자료

### 🔗 **관련 파일**
- **설정**: `src/utils/config.py`
- **로깅**: `src/utils/logger_config.py`
- **파서**: `src/data/parsers.py`
- **시간별 DAG**: `Airflow/dag/weather_data_pipeline.py`
- **주간 DAG**: `Airflow/dag/master_data_update_dag.py`

### 📖 **외부 문서**
- [KMA API 문서](https://www.data.go.kr/data/15084084/openapi.do)
- [AWS S3 파티셔닝 가이드](https://docs.aws.amazon.com/athena/latest/ug/partitions.html)
- [Airflow DAG 가이드](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html)

---

## 🏷️ 버전 정보

- **Pipeline Version**: v3.0
- **최종 업데이트**: 2025-10-01
- **브랜치**: `feat/data-pipeline-finalize-2`
- **저장 방식**: 고정 경로 덮어쓰기 (파티션 구조 제거)
- **Rolling Window**: 21개월 (630일)
- **피처 수**: 34개 (ASOS 9개 필드 전체 포함)

---

**✅ 이 파이프라인으로 안정적이고 확장 가능한 실시간 기상 데이터 수집 및 ML 데이터 준비가 완료됩니다!**