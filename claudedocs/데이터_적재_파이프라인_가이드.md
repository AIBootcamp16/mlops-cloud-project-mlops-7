# ğŸ“Š ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” ê¸°ìƒ ë°ì´í„°(ASOS, PM10) ìˆ˜ì§‘ë¶€í„° ML ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ê¹Œì§€ì˜ ì „ì²´ ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘, S3 ì €ì¥, í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§, Rolling Window ê´€ë¦¬ê¹Œì§€ ëª¨ë“  ê³¼ì •ì´ ìë™í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
[KMA API] â†’ [ë°ì´í„° ìˆ˜ì§‘] â†’ [S3 ì €ì¥] â†’ [í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§] â†’ [ML ë°ì´í„°ì…‹]
    â†“           â†“              â†“                â†“                  â†“
 ASOS/PM10   íŒŒì‹±/ê²€ì¦    íŒŒí‹°ì…˜ êµ¬ì¡°      30ê°œ í”¼ì²˜ ìƒì„±    ì¶”ë¡ ìš©/í•™ìŠµìš© ë¶„ë¦¬
```

### ğŸ“ S3 ë°ì´í„° ê³„ì¸µ êµ¬ì¡°

```
s3://mlops-weather-bucket/
â”œâ”€â”€ raw/                              # ì›ì‹œ ë°ì´í„°
â”‚   â”œâ”€â”€ asos/year=2024/month=09/day=29/153045.txt
â”‚   â””â”€â”€ pm10/year=2024/month=09/day=29/153045.txt
â”œâ”€â”€ processed/                        # íŒŒì‹±ëœ ë°ì´í„°
â”‚   â”œâ”€â”€ asos/year=2024/month=09/day=29/153045.json
â”‚   â””â”€â”€ pm10/year=2024/month=09/day=29/153045.json
â”œâ”€â”€ ml_dataset/                       # ML ë°ì´í„°ì…‹ (Parquet)
â”‚   â”œâ”€â”€ year=2024/month=09/day=29/dataset_153045.parquet           # ì¶”ë¡ ìš©
â”‚   â””â”€â”€ year=2024/month=09/day=29/dataset_153045_training_master.parquet # í•™ìŠµìš©
â””â”€â”€ weather_pm10_integrated_full.csv  # ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ (21ê°œì›” Rolling Window)
```

---

## ğŸ”„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤

### 1ï¸âƒ£ **ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘** (ë§¤ì‹œê°„ 10ë¶„)

**ì‹¤í–‰**: Airflow DAG (`weather_data_pipeline.py`)
**ìŠ¤ì¼€ì¤„**: `'10 * * * *'` (ë§¤ì‹œ 10ë¶„)

```python
# KMA APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
asos_raw = kma_client.fetch_asos(target_time)     # ì§€ìƒê´€ì¸¡ ë°ì´í„°
pm10_raw = kma_client.fetch_pm10(target_time)     # ë¯¸ì„¸ë¨¼ì§€ ë°ì´í„°
```

**ğŸ“ ì°¸ê³ **: UV ë°ì´í„° ìˆ˜ì§‘ì€ í”„ë¡œì íŠ¸ ë²”ìœ„ì—ì„œ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.

### 2ï¸âƒ£ **ë°ì´í„° íŒŒì‹± ë° ê²€ì¦**

```python
# ì›ì‹œ ë°ì´í„° íŒŒì‹±
parsed_asos = parsers.parse_asos_raw(asos_raw)
parsed_pm10 = parsers.parse_pm10_raw(pm10_raw)

# ë°ì´í„° êµ¬ì¡° ì˜ˆì‹œ
{
    "station_id": "100",
    "observed_at": "2024-09-29T15:00:00Z",
    "category": "asos",
    "value": 22.5,
    "unit": "Â°C"
}
```

### 3ï¸âƒ£ **S3 íŒŒí‹°ì…˜ ì €ì¥**

**íŒŒí‹°ì…˜ êµ¬ì¡°**: `year=YYYY/month=MM/day=DD`

```python
# Hive í˜¸í™˜ íŒŒí‹°ì…˜ ìë™ ìƒì„±
partition_path = f"year={year}/month={month}/day={day}"

# ê° ë°ì´í„° íƒ€ì…ë³„ ì €ì¥
raw_key = f"raw/{data_type}/{partition_path}/{timestamp}.txt"
processed_key = f"processed/{data_type}/{partition_path}/{timestamp}.json"
```

**ğŸ¯ ì¥ì **:
- AWS Athena/Glue ìë™ íŒŒí‹°ì…˜ ì¸ì‹
- ë‚ ì§œ ë²”ìœ„ ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™”
- ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆì•½

### 4ï¸âƒ£ **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§** (30ê°œ í”¼ì²˜ ìë™ ìƒì„±)

#### ğŸ“Š ìƒì„±ë˜ëŠ” í”¼ì²˜ ì¹´í…Œê³ ë¦¬

| ì¹´í…Œê³ ë¦¬ | í”¼ì²˜ ìˆ˜ | ì˜ˆì‹œ |
|---------|---------|------|
| **ì‹œê°„ ê¸°ë°˜** | 8ê°œ | `hour`, `day_of_week`, `is_rush_hour`, `season` |
| **ê¸°ì˜¨ ê¸°ë°˜** | 6ê°œ | `temp_category`, `temp_comfort`, `heating_needed` |
| **ì§€ì—­ ê¸°ë°˜** | 4ê°œ | `is_metro_area`, `is_coastal`, `region` |
| **ëŒ€ê¸°ì§ˆ ê¸°ë°˜** | 3ê°œ | `pm10_grade`, `mask_needed`, `outdoor_activity_ok` |
| **ê¸°ë³¸ ë°ì´í„°** | 4ê°œ | `station_id`, `datetime`, `temperature`, `pm10` |
| **ì •ë‹µ ë¼ë²¨** | 1ê°œ | `comfort_score` (í•™ìŠµìš©ë§Œ) |

#### ğŸ”„ ì¶”ë¡ ìš© vs í•™ìŠµìš© ë°ì´í„°ì…‹ ë¶„ë¦¬

```python
# ì¶”ë¡ ìš© ë°ì´í„°ì…‹ (ì‹¤ì‹œê°„ ëª¨ë¸ ì¶”ë¡ )
inference_dataset = create_ml_dataset(raw_data, include_labels=False)
# â†’ comfort_score ì œì™¸, 24ê°œ í”¼ì²˜

# í•™ìŠµìš© ë°ì´í„°ì…‹ (ëª¨ë¸ ì¬í›ˆë ¨)
training_dataset = create_ml_dataset(raw_data, include_labels=True)
# â†’ comfort_score í¬í•¨, 25ê°œ í”¼ì²˜
```

### 5ï¸âƒ£ **Rolling Window ë§ˆìŠ¤í„° ë°ì´í„° ê´€ë¦¬**

#### ğŸ“… **21ê°œì›” ë°ì´í„° ë³´ì¡´** (630ì¼)

```python
# Rolling Window ì ìš©
retention_days = 630  # 21ê°œì›”
cutoff_date = datetime.now() - timedelta(days=retention_days)

# ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì œê±°
merged_df = merged_df[merged_df['datetime'] >= cutoff_date]

# ì¤‘ë³µ ì œê±° (ìµœì‹  ë°ì´í„° ìš°ì„ )
merged_df = merged_df.drop_duplicates(subset=['datetime', 'station_id'], keep='last')
```

#### ğŸ“Š **ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ CSV ì²˜ë¦¬**

```python
# S3ì—ì„œ ê¸°ì¡´ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ
existing_df = weather_handler.load_csv_from_s3('weather_pm10_integrated_full.csv')

# ìƒˆ ë°ì´í„° ë³‘í•© í›„ Rolling Window ì ìš©
updated_df = apply_rolling_window(existing_df, new_data)

# ì—…ë°ì´íŠ¸ëœ ë§ˆìŠ¤í„° ë°ì´í„° ì €ì¥
weather_handler.save_csv_to_s3(updated_df, 'weather_pm10_integrated_full.csv')
```

---

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### ğŸ“¡ **KMAApiClient** (`src/data/kma_client.py`)

```python
class KMAApiClient:
    def fetch_asos(self, target_time) -> str        # ì§€ìƒê´€ì¸¡ ë°ì´í„°
    def fetch_pm10(self, start_time, end_time) -> str  # ë¯¸ì„¸ë¨¼ì§€ ë°ì´í„°
    # fetch_uv() ë©”ì„œë“œëŠ” ì œê±°ë¨
```

### ğŸ­ **WeatherDataProcessor** (`src/data/weather_processor.py`)

```python
class WeatherDataProcessor:
    def process_and_store_weather_data()           # ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
    def update_master_training_dataset()          # Rolling Window ë§ˆìŠ¤í„° ì—…ë°ì´íŠ¸
    def load_latest_ml_dataset()                  # ìµœì‹  ML ë°ì´í„°ì…‹ ë¡œë“œ
```

### ğŸ—„ï¸ **WeatherDataS3Handler** (`src/storage/s3_client.py`)

```python
class WeatherDataS3Handler:
    def save_raw_weather_data()                   # ì›ì‹œ ë°ì´í„° ì €ì¥ (íŒŒí‹°ì…˜)
    def save_parsed_weather_data()                # íŒŒì‹± ë°ì´í„° ì €ì¥ (íŒŒí‹°ì…˜)
    def save_ml_dataset()                         # ML ë°ì´í„°ì…‹ ì €ì¥ (íŒŒí‹°ì…˜)
    def save_csv_to_s3()                          # ë§ˆìŠ¤í„° CSV ì €ì¥
    def load_csv_from_s3()                        # ë§ˆìŠ¤í„° CSV ë¡œë“œ
```

### âš™ï¸ **Feature Builder** (`src/features/feature_builder.py`)

```python
def create_ml_dataset(raw_data, include_labels=False):
    # include_labels=False â†’ ì¶”ë¡ ìš© (comfort_score ì œì™¸)
    # include_labels=True  â†’ í•™ìŠµìš© (comfort_score í¬í•¨)
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1ï¸âƒ£ **ìˆ˜ë™ ì‹¤í–‰** (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)

```bash
cd /path/to/mlops-cloud-project
python src/data/weather_processor.py
```

### 2ï¸âƒ£ **Airflow ìë™í™”** (ìš´ì˜í™˜ê²½)

```bash
# DAG íŒŒì¼ ìœ„ì¹˜
dags/weather_data_pipeline.py

# ìŠ¤ì¼€ì¤„: ë§¤ì‹œê°„ 10ë¶„ (10 * * * *)
# ì˜ˆ: 13:10, 14:10, 15:10, ...
```

### 3ï¸âƒ£ **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

```bash
# KMA API ì„¤ì •
export KMA_API_KEY="your_api_key"
export KMA_BASE_URL="https://apis.data.go.kr/1360000/AsosHourlyInfoService"

# S3 ì„¤ì •
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_REGION="ap-northeast-2"
export S3_BUCKET_NAME="mlops-weather-bucket"
```

---

## ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

### ğŸ” **ë°ì´í„° ê²€ì¦**

- **í˜•ì‹ ê²€ì¦**: JSON ìŠ¤í‚¤ë§ˆ ê²€ì¦, í•„ìˆ˜ í•„ë“œ í™•ì¸
- **ê°’ ê²€ì¦**: ì˜¨ë„ ë²”ìœ„(-50~50Â°C), PM10 ë²”ìœ„(0~999)
- **ì¤‘ë³µ ì œê±°**: ë™ì¼ ì‹œê°„/ê´€ì¸¡ì†Œ ë°ì´í„° ìµœì‹  ìš°ì„ 
- **ëˆ„ë½ ì²˜ë¦¬**: ê²°ì¸¡ê°’ íƒì§€ ë° ë³´ê°„

### ğŸ“ˆ **ëª¨ë‹ˆí„°ë§ ì§€í‘œ**

```python
# ë°ì´í„° ì¸ë²¤í† ë¦¬ í™•ì¸
inventory = weather_handler.get_data_inventory()
{
    "raw_data": 1440,          # ì¼ì¼ ë°ì´í„° ìˆ˜
    "processed_data": 1440,    # ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜
    "ml_datasets": 720,        # ML ë°ì´í„°ì…‹ ìˆ˜
    "master_data": 1,          # ë§ˆìŠ¤í„° íŒŒì¼ ìˆ˜
    "total": 3601              # ì „ì²´ ê°ì²´ ìˆ˜
}
```

---

## ğŸ¯ ML ëª¨ë¸ í™œìš©

### ğŸ”® **ì‹¤ì‹œê°„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸**

```python
# 1. ìµœì‹  ì‹¤ì‹œê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
latest_data = processor.load_latest_ml_dataset(days_back=1)

# 2. ëª¨ë¸ ì¶”ë¡  (comfort_score ì˜ˆì¸¡)
prediction = model.predict(latest_data.drop(['comfort_score'], axis=1))

# 3. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥/ì„œë¹™
```

### ğŸ“ **ëª¨ë¸ ì¬í›ˆë ¨ íŒŒì´í”„ë¼ì¸**

```python
# 1. ë§ˆìŠ¤í„° í•™ìŠµ ë°ì´í„° ë¡œë“œ (21ê°œì›”)
training_data = processor.load_csv_from_s3('weather_pm10_integrated_full.csv')

# 2. í•™ìŠµìš© í”¼ì²˜ ìƒì„± (comfort_score í¬í•¨)
ml_features = create_ml_dataset(training_data, include_labels=True)

# 3. ëª¨ë¸ ì¬í›ˆë ¨ ë° ì„±ëŠ¥ í‰ê°€
model.fit(X_train, y_train)
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### âŒ **ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤**

#### 1ï¸âƒ£ **KMA API ì—°ê²° ì‹¤íŒ¨**
```bash
# í•´ê²°: API í‚¤ ë° ë„¤íŠ¸ì›Œí¬ í™•ì¸
export KMA_API_KEY="ì˜¬ë°”ë¥¸_API_í‚¤"
curl -I https://apis.data.go.kr/1360000/AsosHourlyInfoService
```

#### 2ï¸âƒ£ **S3 ê¶Œí•œ ì˜¤ë¥˜**
```bash
# í•´ê²°: IAM ê¶Œí•œ í™•ì¸
aws s3 ls s3://mlops-weather-bucket/
```

#### 3ï¸âƒ£ **íŒŒí‹°ì…˜ ì¸ì‹ ì•ˆë¨**
```sql
-- í•´ê²°: Athenaì—ì„œ íŒŒí‹°ì…˜ ìˆ˜ë™ ì¶”ê°€
MSCK REPAIR TABLE weather_data;
```

#### 4ï¸âƒ£ **Rolling Window ë°ì´í„° ë¶€ì¡±**
```python
# í•´ê²°: retention_days ì¡°ì •
processor.update_master_training_dataset(
    new_data, retention_days=365  # 12ê°œì›”ë¡œ ë‹¨ì¶•
)
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ğŸ”— **ê´€ë ¨ íŒŒì¼**
- **ì„¤ì •**: `src/utils/config.py`
- **ë¡œê¹…**: `src/utils/logger_config.py`
- **íŒŒì„œ**: `src/data/parsers.py`
- **DAG**: `dags/weather_data_pipeline.py`

### ğŸ“– **ì™¸ë¶€ ë¬¸ì„œ**
- [KMA API ë¬¸ì„œ](https://www.data.go.kr/data/15084084/openapi.do)
- [AWS S3 íŒŒí‹°ì…”ë‹ ê°€ì´ë“œ](https://docs.aws.amazon.com/athena/latest/ug/partitions.html)
- [Airflow DAG ê°€ì´ë“œ](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html)

---

## ğŸ·ï¸ ë²„ì „ ì •ë³´

- **Pipeline Version**: v2.0
- **ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-09-29
- **ë¸Œëœì¹˜**: `feat/update-latest-main-data-pipeline`
- **Rolling Window**: 21ê°œì›” (630ì¼)
- **í”¼ì²˜ ìˆ˜**: 30ê°œ (ì¶”ë¡ ìš© 24ê°œ + í•™ìŠµìš© 1ê°œ)

---

**âœ… ì´ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ë° ML ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œë©ë‹ˆë‹¤!**