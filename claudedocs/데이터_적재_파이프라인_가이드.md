# 📊 데이터 적재 파이프라인 가이드

## 📋 개요

이 문서는 기상 데이터(ASOS, PM10) 수집부터 ML 모델 학습용 데이터 준비까지의 전체 데이터 적재 파이프라인을 설명합니다. 실시간 데이터 수집, S3 저장, 피처 엔지니어링, Rolling Window 관리까지 모든 과정이 자동화되어 있습니다.

---

## 🏗️ 시스템 아키텍처

```
[KMA API] → [데이터 수집] → [S3 저장] → [피처 엔지니어링] → [ML 데이터셋]
    ↓           ↓              ↓                ↓                  ↓
 ASOS/PM10   파싱/검증    파티션 구조      30개 피처 생성    추론용/학습용 분리
```

### 📁 S3 데이터 계층 구조

```
s3://mlops-weather-bucket/
├── raw/                              # 원시 데이터
│   ├── asos/year=2024/month=09/day=29/153045.txt
│   └── pm10/year=2024/month=09/day=29/153045.txt
├── processed/                        # 파싱된 데이터
│   ├── asos/year=2024/month=09/day=29/153045.json
│   └── pm10/year=2024/month=09/day=29/153045.json
├── ml_dataset/                       # ML 데이터셋 (Parquet)
│   ├── year=2024/month=09/day=29/dataset_153045.parquet           # 추론용
│   └── year=2024/month=09/day=29/dataset_153045_training_master.parquet # 학습용
└── weather_pm10_integrated_full.csv  # 마스터 데이터셋 (21개월 Rolling Window)
```

---

## 🔄 데이터 파이프라인 프로세스

### 1️⃣ **실시간 데이터 수집** (매시간 10분)

**실행**: Airflow DAG (`weather_data_pipeline.py`)
**스케줄**: `'10 * * * *'` (매시 10분)

```python
# KMA API에서 데이터 수집
asos_raw = kma_client.fetch_asos(target_time)     # 지상관측 데이터
pm10_raw = kma_client.fetch_pm10(target_time)     # 미세먼지 데이터
```

**📍 참고**: UV 데이터 수집은 프로젝트 범위에서 제외되었습니다.

### 2️⃣ **데이터 파싱 및 검증**

```python
# 원시 데이터 파싱
parsed_asos = parsers.parse_asos_raw(asos_raw)
parsed_pm10 = parsers.parse_pm10_raw(pm10_raw)

# 데이터 구조 예시
{
    "station_id": "100",
    "observed_at": "2024-09-29T15:00:00Z",
    "category": "asos",
    "value": 22.5,
    "unit": "°C"
}
```

### 3️⃣ **S3 파티션 저장**

**파티션 구조**: `year=YYYY/month=MM/day=DD`

```python
# Hive 호환 파티션 자동 생성
partition_path = f"year={year}/month={month}/day={day}"

# 각 데이터 타입별 저장
raw_key = f"raw/{data_type}/{partition_path}/{timestamp}.txt"
processed_key = f"processed/{data_type}/{partition_path}/{timestamp}.json"
```

**🎯 장점**:
- AWS Athena/Glue 자동 파티션 인식
- 날짜 범위 쿼리 성능 최적화
- 스토리지 비용 절약

### 4️⃣ **피처 엔지니어링** (30개 피처 자동 생성)

#### 📊 생성되는 피처 카테고리

| 카테고리 | 피처 수 | 예시 |
|---------|---------|------|
| **시간 기반** | 8개 | `hour`, `day_of_week`, `is_rush_hour`, `season` |
| **기온 기반** | 6개 | `temp_category`, `temp_comfort`, `heating_needed` |
| **지역 기반** | 4개 | `is_metro_area`, `is_coastal`, `region` |
| **대기질 기반** | 3개 | `pm10_grade`, `mask_needed`, `outdoor_activity_ok` |
| **기본 데이터** | 4개 | `station_id`, `datetime`, `temperature`, `pm10` |
| **정답 라벨** | 1개 | `comfort_score` (학습용만) |

#### 🔄 추론용 vs 학습용 데이터셋 분리

```python
# 추론용 데이터셋 (실시간 모델 추론)
inference_dataset = create_ml_dataset(raw_data, include_labels=False)
# → comfort_score 제외, 24개 피처

# 학습용 데이터셋 (모델 재훈련)
training_dataset = create_ml_dataset(raw_data, include_labels=True)
# → comfort_score 포함, 25개 피처
```

### 5️⃣ **Rolling Window 마스터 데이터 관리**

#### 📅 **21개월 데이터 보존** (630일)

```python
# Rolling Window 적용
retention_days = 630  # 21개월
cutoff_date = datetime.now() - timedelta(days=retention_days)

# 오래된 데이터 자동 제거
merged_df = merged_df[merged_df['datetime'] >= cutoff_date]

# 중복 제거 (최신 데이터 우선)
merged_df = merged_df.drop_duplicates(subset=['datetime', 'station_id'], keep='last')
```

#### 📊 **마스터 데이터셋 CSV 처리**

```python
# S3에서 기존 마스터 데이터 로드
existing_df = weather_handler.load_csv_from_s3('weather_pm10_integrated_full.csv')

# 새 데이터 병합 후 Rolling Window 적용
updated_df = apply_rolling_window(existing_df, new_data)

# 업데이트된 마스터 데이터 저장
weather_handler.save_csv_to_s3(updated_df, 'weather_pm10_integrated_full.csv')
```

---

## 🔧 핵심 컴포넌트

### 📡 **KMAApiClient** (`src/data/kma_client.py`)

```python
class KMAApiClient:
    def fetch_asos(self, target_time) -> str        # 지상관측 데이터
    def fetch_pm10(self, start_time, end_time) -> str  # 미세먼지 데이터
    # fetch_uv() 메서드는 제거됨
```

### 🏭 **WeatherDataProcessor** (`src/data/weather_processor.py`)

```python
class WeatherDataProcessor:
    def process_and_store_weather_data()           # 실시간 데이터 처리
    def update_master_training_dataset()          # Rolling Window 마스터 업데이트
    def load_latest_ml_dataset()                  # 최신 ML 데이터셋 로드
```

### 🗄️ **WeatherDataS3Handler** (`src/storage/s3_client.py`)

```python
class WeatherDataS3Handler:
    def save_raw_weather_data()                   # 원시 데이터 저장 (파티션)
    def save_parsed_weather_data()                # 파싱 데이터 저장 (파티션)
    def save_ml_dataset()                         # ML 데이터셋 저장 (파티션)
    def save_csv_to_s3()                          # 마스터 CSV 저장
    def load_csv_from_s3()                        # 마스터 CSV 로드
```

### ⚙️ **Feature Builder** (`src/features/feature_builder.py`)

```python
def create_ml_dataset(raw_data, include_labels=False):
    # include_labels=False → 추론용 (comfort_score 제외)
    # include_labels=True  → 학습용 (comfort_score 포함)
```

---

## 🚀 실행 방법

### 1️⃣ **수동 실행** (개발/테스트용)

```bash
cd /path/to/mlops-cloud-project
python src/data/weather_processor.py
```

### 2️⃣ **Airflow 자동화** (운영환경)

```bash
# DAG 파일 위치
dags/weather_data_pipeline.py

# 스케줄: 매시간 10분 (10 * * * *)
# 예: 13:10, 14:10, 15:10, ...
```

### 3️⃣ **환경 변수 설정**

```bash
# KMA API 설정
export KMA_API_KEY="your_api_key"
export KMA_BASE_URL="https://apis.data.go.kr/1360000/AsosHourlyInfoService"

# S3 설정
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_REGION="ap-northeast-2"
export S3_BUCKET_NAME="mlops-weather-bucket"
```

---

## 📊 데이터 품질 관리

### 🔍 **데이터 검증**

- **형식 검증**: JSON 스키마 검증, 필수 필드 확인
- **값 검증**: 온도 범위(-50~50°C), PM10 범위(0~999)
- **중복 제거**: 동일 시간/관측소 데이터 최신 우선
- **누락 처리**: 결측값 탐지 및 보간

### 📈 **모니터링 지표**

```python
# 데이터 인벤토리 확인
inventory = weather_handler.get_data_inventory()
{
    "raw_data": 1440,          # 일일 데이터 수
    "processed_data": 1440,    # 처리된 데이터 수
    "ml_datasets": 720,        # ML 데이터셋 수
    "master_data": 1,          # 마스터 파일 수
    "total": 3601              # 전체 객체 수
}
```

---

## 🎯 ML 모델 활용

### 🔮 **실시간 추론 파이프라인**

```python
# 1. 최신 실시간 데이터 가져오기
latest_data = processor.load_latest_ml_dataset(days_back=1)

# 2. 모델 추론 (comfort_score 예측)
prediction = model.predict(latest_data.drop(['comfort_score'], axis=1))

# 3. 예측 결과 저장/서빙
```

### 🎓 **모델 재훈련 파이프라인**

```python
# 1. 마스터 학습 데이터 로드 (21개월)
training_data = processor.load_csv_from_s3('weather_pm10_integrated_full.csv')

# 2. 학습용 피처 생성 (comfort_score 포함)
ml_features = create_ml_dataset(training_data, include_labels=True)

# 3. 모델 재훈련 및 성능 평가
model.fit(X_train, y_train)
```

---

## 🔧 트러블슈팅

### ❌ **자주 발생하는 문제들**

#### 1️⃣ **KMA API 연결 실패**
```bash
# 해결: API 키 및 네트워크 확인
export KMA_API_KEY="올바른_API_키"
curl -I https://apis.data.go.kr/1360000/AsosHourlyInfoService
```

#### 2️⃣ **S3 권한 오류**
```bash
# 해결: IAM 권한 확인
aws s3 ls s3://mlops-weather-bucket/
```

#### 3️⃣ **파티션 인식 안됨**
```sql
-- 해결: Athena에서 파티션 수동 추가
MSCK REPAIR TABLE weather_data;
```

#### 4️⃣ **Rolling Window 데이터 부족**
```python
# 해결: retention_days 조정
processor.update_master_training_dataset(
    new_data, retention_days=365  # 12개월로 단축
)
```

---

## 📚 참고 자료

### 🔗 **관련 파일**
- **설정**: `src/utils/config.py`
- **로깅**: `src/utils/logger_config.py`
- **파서**: `src/data/parsers.py`
- **DAG**: `dags/weather_data_pipeline.py`

### 📖 **외부 문서**
- [KMA API 문서](https://www.data.go.kr/data/15084084/openapi.do)
- [AWS S3 파티셔닝 가이드](https://docs.aws.amazon.com/athena/latest/ug/partitions.html)
- [Airflow DAG 가이드](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html)

---

## 🏷️ 버전 정보

- **Pipeline Version**: v2.0
- **최종 업데이트**: 2024-09-29
- **브랜치**: `feat/update-latest-main-data-pipeline`
- **Rolling Window**: 21개월 (630일)
- **피처 수**: 30개 (추론용 24개 + 학습용 1개)

---

**✅ 이 파이프라인으로 안정적이고 확장 가능한 실시간 기상 데이터 수집 및 ML 데이터 준비가 완료됩니다!**