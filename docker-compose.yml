services:
  # FastAPI 서버 (services/api 폴더 기반)
  api-server:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    working_dir: /app
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_BUCKET=${S3_BUCKET}
      - WEATHER_API_KEY=${WEATHER_API_KEY}
      - MYSQL_HOST=mysql
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD:-mlops2025}
      - MYSQL_DATABASE=weather_mlops
    command: uvicorn services.api.main:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - ml-network
    depends_on:
      - mysql

  # Next.js (React) 프론트엔드 (frontend 폴더 기반)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    networks:
      - ml-network
    depends_on:
      - api-server

  # 데이터 수집/처리 서비스
  data-processor:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile
    volumes:
      - .:/app
    working_dir: /app
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_BUCKET=${S3_BUCKET}
      - WEATHER_API_KEY=${WEATHER_API_KEY}
    command: python src/data/weather_collector.py
    networks:
      - ml-network

  # Jupyter Lab (notebooks 폴더 활용)
  jupyter:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile.jupyter
    ports:
      - "8888:8888"
    volumes:
      - .:/app
    working_dir: /app
    env_file: .env
    environment:
      - WEATHER_API_KEY=${WEATHER_API_KEY}
      - WANDB_MODE=${WANDB_MODE}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - S3_BUCKET=${S3_BUCKET}
      - MYSQL_HOST=${MYSQL_HOST}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - CHAMPION_MODEL=${CHAMPION_MODEL}
      - WANDB_API_KEY=${WANDB_API_KEY}
      - WANDB_ENTITY=${WANDB_ENTITY}
      - WANDB_PROJECT=${WANDB_PROJECT}
      - WANDB_DIR=/tmp/wandb

    networks:
      - ml-network

  # MySQL 데이터베이스
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD} # 기본값 제거
      MYSQL_DATABASE: weather_mlops
      TZ: Asia/Seoul
    command: --default-time-zone='+09:00'
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    # ports 제거 권장 (내부 통신만)
    networks:
      - ml-network
    restart: unless-stopped

  # phpMyAdmin (웹 기반 MySQL 관리)
  phpmyadmin:
    image: phpmyadmin:latest
    environment:
      PMA_HOST: mysql
      PMA_PORT: 3306
    ports:
      - "8081:80"
    networks:
      - ml-network
    depends_on:
      - mysql

  airflow-postgres:
    image: postgres:14
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ml-network

  airflow-init:
    image: apache/airflow:2.8.4-python3.11
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      - AIRFLOW_UID=50000
      - PYTHONPATH=/opt/airflow
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - S3_BUCKET=${S3_BUCKET}
      - WEATHER_API_KEY=${WEATHER_API_KEY}
      - WANDB_API_KEY=${WANDB_API_KEY}
      - WANDB_ENTITY=${WANDB_ENTITY}
      - WANDB_PROJECT=${WANDB_PROJECT}
      - WANDB_MODE=${WANDB_MODE}
    user: "0:0"
    entrypoint: [ "bash", "-c" ]
    command:
      - >-
        apt-get update && apt-get install -y libgomp1 && su airflow -c "pip install --no-cache-dir -r /opt/airflow/requirements.txt" && su airflow -c "airflow db init" && su airflow -c "airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin" || true
    volumes:
      - ./Airflow/dag:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./requirements.txt:/opt/airflow/requirements.txt
    networks:
      - ml-network

  airflow-webserver:
    image: apache/airflow:2.8.4-python3.11
    depends_on:
      airflow-postgres:
        condition: service_healthy
      airflow-init:
        condition: service_started
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW_UID=50000
      - PYTHONPATH=/opt/airflow
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - S3_BUCKET=${S3_BUCKET}
      - WEATHER_API_KEY=${WEATHER_API_KEY}
      - WANDB_API_KEY=${WANDB_API_KEY}
      - WANDB_ENTITY=${WANDB_ENTITY}
      - WANDB_PROJECT=${WANDB_PROJECT}
      - WANDB_MODE=${WANDB_MODE}
    user: "0:0"
    entrypoint: [ "bash", "-c" ]
    command:
      - >-
        apt-get update && apt-get install -y libgomp1 && su airflow -c "pip install --no-cache-dir -r /opt/airflow/requirements.txt" && su airflow -c "airflow webserver"
    ports:
      - "8080:8080"
    volumes:
      - ./Airflow/dag:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./requirements.txt:/opt/airflow/requirements.txt
    networks:
      - ml-network

  airflow-scheduler:
    image: apache/airflow:2.8.4-python3.11
    depends_on:
      airflow-postgres:
        condition: service_healthy
      airflow-init:
        condition: service_started
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW_UID=50000
      - PYTHONPATH=/opt/airflow
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - S3_BUCKET=${S3_BUCKET}
      - WEATHER_API_KEY=${WEATHER_API_KEY}
      - WANDB_API_KEY=${WANDB_API_KEY}
      - WANDB_ENTITY=${WANDB_ENTITY}
      - WANDB_PROJECT=${WANDB_PROJECT}
      - WANDB_MODE=${WANDB_MODE}
    user: "0:0"
    entrypoint: [ "bash", "-c" ]
    command:
      - >-
        apt-get update && apt-get install -y libgomp1 && su airflow -c "pip install --no-cache-dir -r /opt/airflow/requirements.txt" && su airflow -c "airflow scheduler"
    volumes:
      - ./Airflow/dag:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./requirements.txt:/opt/airflow/requirements.txt
    networks:
      - ml-network

networks:
  ml-network:
    driver: bridge

volumes:
  mysql_data:
  airflow-postgres-data:
