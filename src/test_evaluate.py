import yaml
import numpy as np
import os
from dotenv import load_dotenv
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# wandb importë¥¼ optionalë¡œ ì²˜ë¦¬
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    wandb = None
    WANDB_AVAILABLE = False

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì‹œìŠ¤í…œ ê²½ë¡œ ì„¤ì •
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from models.registry import get_model
from models.evaluate import ModelEvaluator
from test_integration import test_with_yaml_config

def generate_run_name():
    """run name ìë™ ìƒì„±"""
    import time
    timestamp = int(time.time())
    return f"test_eval_{timestamp}"

def init_wandb(use_wandb=True, project_name="test-evaluation"):
    """wandb ì´ˆê¸°í™”"""
    if not use_wandb:
        return False
        
    try:
        # wandbê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
        if not WANDB_AVAILABLE:
            print("âš ï¸ wandbê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. wandb ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            return False
        
        # .envì—ì„œ API í‚¤ ë¡œë“œ
        api_key = os.getenv('WANDB_API_KEY')
        if not api_key:
            print("âš ï¸ WANDB_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        # wandb ì´ˆê¸°í™”
        run_name = generate_run_name()
        wandb.init(
            project=project_name,
            name=run_name,
            config={
                'test_type': 'model_evaluation',
                'framework': 'sklearn'
            }
        )
        print(f"ğŸ“Š wandb ì´ˆê¸°í™” ì™„ë£Œ - Project: {project_name}, Run: {run_name}")
        return True
        
    except Exception as e:
        print(f"âš ï¸ wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        print("wandb ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return False

def log_test_results_to_wandb(test_results, use_wandb=False):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ wandbì— ë¡œê¹…"""
    if not use_wandb or not WANDB_AVAILABLE:
        return
    
    try:
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…
        passed = sum(test_results.values())
        total = len(test_results)
        success_rate = passed / total * 100 if total > 0 else 0
        
        wandb.log({
            'total_tests': total,
            'passed_tests': passed,
            'failed_tests': total - passed,
            'success_rate': success_rate
        })
        
        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…
        for test_name, result in test_results.items():
            wandb.log({f"test_{test_name.replace(' ', '_').lower()}": 1 if result else 0})
        
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ wandbì— ë¡œê¹…ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âš ï¸ wandb ë¡œê¹… ì‹¤íŒ¨: {str(e)}")

def get_trained_models_from_integration():
    """test_integration.pyì—ì„œ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ ê°€ì ¸ì˜¤ê¸°"""
    print("ğŸ“¦ test_integration.pyì—ì„œ í•™ìŠµëœ ëª¨ë¸ë“¤ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    
    # test_integration.py ì‹¤í–‰í•˜ì—¬ í•™ìŠµëœ ëª¨ë¸ë“¤ê³¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    import importlib
    import test_integration as test_integration
    importlib.reload(test_integration)  # ëª¨ë“ˆ ì¬ë¡œë“œ
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (test_integration.pyì™€ ë™ì¼)
    from sklearn.datasets import make_regression
    from sklearn.model_selection import train_test_split
    
    X, y = make_regression(n_samples=200, n_features=10, noise=0.1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # test_integration.pyì˜ ì„¤ì • ë¡œë“œ
    try:
        with open('config/models.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print("âœ… ì‹¤ì œ YAML ì„¤ì • íŒŒì¼ ì‚¬ìš©")
    except FileNotFoundError:
        print("âš ï¸ YAML íŒŒì¼ì´ ì—†ì–´ì„œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        config = {
            'task': 'regression',
            'target': 'price',
            'cv': {'n_splits': 5, 'shuffle': True, 'random_state': 42},
            'metrics': {'primary': 'rmse', 'others': ['mae', 'r2']},
            'models': [
                {'name': 'linear', 'params': {}},
                {'name': 'ridge', 'params': {'alpha': 1.0}},
                {'name': 'rf', 'params': {'n_estimators': 50, 'random_state': 42}}
            ]
        }
    
    # ëª¨ë¸ í•™ìŠµ (test_integration.py ë°©ì‹ê³¼ ë™ì¼)
    trained_models = {}
    
    for model_config in config['models']:
        name = model_config['name']
        params = model_config['params']
        
        try:
            print(f"ğŸ”„ {name} ëª¨ë¸ ì¬í•™ìŠµ ì¤‘...")
            model = get_model(config['task'], name, params)
            model.fit(X_train, y_train)
            trained_models[name] = model
            print(f"âœ… {name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    return trained_models, X_test, y_test, config

def test_regression_evaluation():
    """íšŒê·€ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("=== íšŒê·€ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ===")
    
    # test_integration.pyì—ì„œ í•™ìŠµëœ ëª¨ë¸ë“¤ ê°€ì ¸ì˜¤ê¸°
    trained_models, X_test, y_test, config = get_trained_models_from_integration()
    
    if not trained_models:
        print("âŒ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ì‹¤ì œ ì„¤ì • íŒŒì¼ ì‚¬ìš© (ì„ì‹œ íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ)
    config_path = 'config/models.yaml'
    
    try:
        # ModelEvaluator í…ŒìŠ¤íŠ¸ (wandb í™œì„±í™”)
        print("\nğŸ” ModelEvaluator ì´ˆê¸°í™”...")
        evaluator = ModelEvaluator(config_path=config_path, use_wandb=True, project_name="test-model-evaluation")
        print("âœ… ModelEvaluator ì´ˆê¸°í™” ì„±ê³µ")
        
        # ëª¨ë¸ í‰ê°€
        print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        evaluator.evaluate_models(trained_models, X_test, y_test)
        print("âœ… ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
        
        # ê²°ê³¼ í™•ì¸
        print("\nğŸ“‹ ê²°ê³¼ í™•ì¸...")
        summary_df = evaluator.get_summary_dataframe()
        if summary_df is not None:
            print("âœ… ìš”ì•½ DataFrame ìƒì„± ì„±ê³µ")
            print(summary_df)
        else:
            print("âŒ ìš”ì•½ DataFrame ìƒì„± ì‹¤íŒ¨")
            return False
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™•ì¸
        best_model = evaluator.get_best_model()
        if best_model:
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['name']}")
            print("âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ ì„±ê³µ")
        else:
            print("âŒ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ ì‹¤íŒ¨")
            return False
        
        # ì‹œê°í™” í…ŒìŠ¤íŠ¸ (ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸)
        print("\nğŸ“ˆ ì‹œê°í™” í…ŒìŠ¤íŠ¸...")
        try:
            # plot_resultsëŠ” ì‹¤ì œë¡œ plt.show()ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
            # evaluator.plot_results()
            print("âœ… ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ ê°€ëŠ¥ (ì‹¤ì œ ì‹¤í–‰ì€ ìƒëµ)")
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ íšŒê·€ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def get_trained_classification_models():
    """ë¶„ë¥˜ìš© í•™ìŠµëœ ëª¨ë¸ë“¤ ê°€ì ¸ì˜¤ê¸°"""
    print("ğŸ“¦ ë¶„ë¥˜ ëª¨ë¸ë“¤ í•™ìŠµ ì¤‘...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ë¶„ë¥˜ìš©)
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    
    X, y = make_classification(n_samples=200, n_features=10, n_classes=2, 
                             n_informative=5, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # ë¶„ë¥˜ ì„¤ì •
    config = {
        'task': 'classification',
        'target': 'class',
        'cv': {'n_splits': 5, 'shuffle': True, 'random_state': 42},
        'metrics': {'primary': 'accuracy', 'others': ['precision', 'recall', 'f1']},
        'models': [
            {'name': 'logistic_regression', 'params': {'random_state': 42, 'max_iter': 200}},
            {'name': 'ridge_classifier', 'params': {'random_state': 42}},
            {'name': 'rf', 'params': {'n_estimators': 50, 'random_state': 42}}
        ]
    }
    
    # ëª¨ë¸ í•™ìŠµ
    trained_models = {}
    
    for model_config in config['models']:
        name = model_config['name']
        params = model_config['params']
        
        try:
            print(f"ğŸ”„ {name} ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì¤‘...")
            model = get_model(config['task'], name, params)
            model.fit(X_train, y_train)
            trained_models[name] = model
            print(f"âœ… {name} ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {name} ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    return trained_models, X_test, y_test, config

def test_classification_evaluation():
    """ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("\n=== ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ===")
    
    # ë¶„ë¥˜ ëª¨ë¸ë“¤ ê°€ì ¸ì˜¤ê¸°
    trained_models, X_test, y_test, config = get_trained_classification_models()
    
    if not trained_models:
        print("âŒ í•™ìŠµëœ ë¶„ë¥˜ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ì„ì‹œ ë¶„ë¥˜ ì„¤ì • íŒŒì¼ ìƒì„±
    temp_config_path = 'temp_classification_config.yaml'
    with open(temp_config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True)
    
    try:
        # ModelEvaluator í…ŒìŠ¤íŠ¸ (ë¶„ë¥˜ìš©)
        print("\nğŸ” ë¶„ë¥˜ ModelEvaluator ì´ˆê¸°í™”...")
        evaluator = ModelEvaluator(config_path=temp_config_path, use_wandb=True, project_name="test-classification-evaluation")
        print("âœ… ë¶„ë¥˜ ModelEvaluator ì´ˆê¸°í™” ì„±ê³µ")
        
        # ëª¨ë¸ í‰ê°€
        print("\nğŸ“Š ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        evaluator.evaluate_models(trained_models, X_test, y_test)
        print("âœ… ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
        
        # ê²°ê³¼ í™•ì¸
        print("\nğŸ“‹ ë¶„ë¥˜ ê²°ê³¼ í™•ì¸...")
        summary_df = evaluator.get_summary_dataframe()
        if summary_df is not None:
            print("âœ… ë¶„ë¥˜ ìš”ì•½ DataFrame ìƒì„± ì„±ê³µ")
            print(summary_df)
        else:
            print("âŒ ë¶„ë¥˜ ìš”ì•½ DataFrame ìƒì„± ì‹¤íŒ¨")
            return False
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™•ì¸
        best_model = evaluator.get_best_model()
        if best_model:
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ë¶„ë¥˜ ëª¨ë¸: {best_model['name']}")
            print("âœ… ìµœê³  ì„±ëŠ¥ ë¶„ë¥˜ ëª¨ë¸ ì„ íƒ ì„±ê³µ")
        else:
            print("âŒ ìµœê³  ì„±ëŠ¥ ë¶„ë¥˜ ëª¨ë¸ ì„ íƒ ì‹¤íŒ¨")
            return False
        
        # ë¶„ë¥˜ ì‹œê°í™” í…ŒìŠ¤íŠ¸
        print("\nğŸ“ˆ ë¶„ë¥˜ ì‹œê°í™” í…ŒìŠ¤íŠ¸...")
        try:
            # plot_resultsëŠ” ì‹¤ì œë¡œ plt.show()ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
            # evaluator.plot_results()
            print("âœ… ë¶„ë¥˜ ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ ê°€ëŠ¥ (ì‹¤ì œ ì‹¤í–‰ì€ ìƒëµ)")
        except Exception as e:
            print(f"âš ï¸ ë¶„ë¥˜ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_config_path):
            os.remove(temp_config_path)
            print(f"ğŸ—‘ï¸ ì„ì‹œ ë¶„ë¥˜ ì„¤ì • íŒŒì¼ ì‚­ì œ: {temp_config_path}")

def test_yaml_config_loading():
    """ì‹¤ì œ YAML ì„¤ì • íŒŒì¼ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n=== ì‹¤ì œ YAML ì„¤ì • íŒŒì¼ í…ŒìŠ¤íŠ¸ ===")
    
    config_path = 'config/models.yaml'
    
    try:
        evaluator = ModelEvaluator(config_path=config_path, use_wandb=False)
        print(f"âœ… ì‹¤ì œ ì„¤ì • íŒŒì¼ ë¡œë”© ì„±ê³µ: {config_path}")
        print(f"Task: {evaluator.task}")
        print(f"Target: {evaluator.target}")
        print(f"CV Splits: {evaluator.cv_config['n_splits']}")
        print(f"Primary Metric: {evaluator.metrics_config['primary']}")
        return True
    except FileNotFoundError:
        print(f"âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        return False
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

def test_error_handling():
    """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ===")
    
    # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì„¤ì • íŒŒì¼
    try:
        evaluator = ModelEvaluator(config_path='nonexistent.yaml', use_wandb=False)
        print("âŒ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì— ëŒ€í•œ ì—ëŸ¬ ì²˜ë¦¬ ì‹¤íŒ¨")
        return False
    except FileNotFoundError:
        print("âœ… ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì— ëŒ€í•œ ì—ëŸ¬ ì²˜ë¦¬ ì„±ê³µ")
    
    # ë¹ˆ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ë¡œ í‰ê°€ ì‹œë„
    try:
        evaluator = ModelEvaluator(config_path='config/models.yaml', use_wandb=False)
        X, y = make_regression(n_samples=50, n_features=3, random_state=42)
        
        # ë¹ˆ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        evaluator.evaluate_models({}, X, y)
        print("âœ… ë¹ˆ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ì— ëŒ€í•œ ì²˜ë¦¬ ì„±ê³µ")
        
        # ê²°ê³¼ í™•ì¸
        summary_df = evaluator.get_summary_dataframe()
        best_model = evaluator.get_best_model()
        
        if summary_df is None and best_model is None:
            print("âœ… ë¹ˆ ê²°ê³¼ì— ëŒ€í•œ ì²˜ë¦¬ ì„±ê³µ")
        else:
            print("âŒ ë¹ˆ ê²°ê³¼ì— ëŒ€í•œ ì²˜ë¦¬ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    return True

def run_all_tests(use_wandb=True):
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ ModelEvaluator í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # wandb ì´ˆê¸°í™”
    wandb_enabled = init_wandb(use_wandb=use_wandb, project_name="test-evaluation")
    
    tests = [
        ("YAML ì„¤ì • ë¡œë”©", test_yaml_config_loading),
        ("íšŒê·€ ëª¨ë¸ í‰ê°€", test_regression_evaluation),
        ("ë¶„ë¥˜ ëª¨ë¸ í‰ê°€", test_classification_evaluation),
        ("ì—ëŸ¬ ì²˜ë¦¬", test_error_handling)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            print(f"\nğŸ§ª {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
            results[test_name] = test_func()
            if results[test_name]:
                print(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                print(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        except Exception as e:
            print(f"ğŸ’¥ {test_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            results[test_name] = False
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 50)
    print("ğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{test_name:20} | {status}")
    
    print("-" * 50)
    print(f"ì´ í…ŒìŠ¤íŠ¸: {total}")
    print(f"ì„±ê³µ: {passed}")
    print(f"ì‹¤íŒ¨: {total - passed}")
    print(f"ì„±ê³µë¥ : {passed/total*100:.1f}%")
    
    if passed == total:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâš ï¸ {total - passed}ê°œì˜ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # wandbì— í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…
    log_test_results_to_wandb(results, use_wandb=wandb_enabled)
    
    # wandb ì¢…ë£Œ
    if wandb_enabled and WANDB_AVAILABLE:
        try:
            wandb.finish()
            print("ğŸ“Š wandb ì‹¤í–‰ ì¢…ë£Œë¨")
        except:
            pass
    
    return passed == total

if __name__ == "__main__":
    # wandb ì‚¬ìš© ì—¬ë¶€ ê²°ì • (í™˜ê²½ë³€ìˆ˜ë‚˜ ì¸ìë¡œ ì œì–´ ê°€ëŠ¥)
    use_wandb = os.getenv('USE_WANDB', 'true').lower() == 'true'
    success = run_all_tests(use_wandb=use_wandb)
    exit(0 if success else 1) 