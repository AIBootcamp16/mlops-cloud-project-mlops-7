import numpy as np
import pandas as pd
import os
from dotenv import load_dotenv

# wandb importë¥¼ optionalë¡œ ì²˜ë¦¬
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    wandb = None
    WANDB_AVAILABLE = False

# matplotlib ì„¤ì •ì„ import ì „ì— ë¨¼ì € ì„¤ì •
import matplotlib
matplotlib.use('Agg')  # GUI ë°±ì—”ë“œ ë¹„í™œì„±í™”
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì • (import ì§í›„)
plt.rcParams['font.family'] = ['Noto Sans CJK KR', 'Noto Sans CJK JP', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

import seaborn as sns
import yaml
from pathlib import Path
from sklearn.model_selection import cross_val_score
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score, roc_curve
)
from sklearn.datasets import make_regression, make_classification
import warnings
warnings.filterwarnings('ignore')

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì‹œìŠ¤í…œ ê²½ë¡œ ì„¤ì •
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

def generate_run_name():
    """run name ìë™ ìƒì„±"""
    import time
    timestamp = int(time.time())
    return f"model_eval_{timestamp}"

class ModelEvaluator:
    """í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ë° ì‹œê°í™” ì „ìš© í´ë˜ìŠ¤ (wandb í†µí•©)"""
    
    def __init__(self, config_path='conf/models.yaml', use_wandb=False, project_name="model-evaluation"):
        """
        Args:
            config_path: ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
            use_wandb: wandb ì‚¬ìš© ì—¬ë¶€
            project_name: wandb í”„ë¡œì íŠ¸ ì´ë¦„
        """
        self.config = self._load_config(config_path)
        self.task = self.config['task']
        self.target = self.config['target']
        self.cv_config = self.config['cv']
        self.metrics_config = self.config['metrics']
        
        self.results = {}
        self.trained_models = {}
        
        # wandb ì„¤ì • (í˜ì´ì§€ ë°©ì‹ ë”°ë¼ ë‹¨ìˆœí™”)
        self.use_wandb = use_wandb
        if self.use_wandb:
            self._init_wandb(project_name)
    
    def _init_wandb(self, project_name):
        """wandb ì´ˆê¸°í™” - í˜ì´ì§€ ë°©ì‹ ë”°ë¼ ë‹¨ìˆœí™”"""
        try:
            # wandbê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
            if not WANDB_AVAILABLE:
                print("âš ï¸ wandbê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. wandb ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
                self.use_wandb = False
                return
            
            # .envì—ì„œ API í‚¤ ë¡œë“œ
            api_key = os.getenv('WANDB_API_KEY')
            if not api_key:
                print("âš ï¸ WANDB_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                self.use_wandb = False
                return
            
            # wandb ì´ˆê¸°í™” (í˜ì´ì§€ì˜ ê°„ë‹¨í•œ ë°©ì‹ ì‚¬ìš©)
            run_name = generate_run_name()
            wandb.init(
                project=project_name,
                name=run_name,
                config={
                    'task': self.task,
                    'target': self.target,
                    'cv_splits': self.cv_config['n_splits'],
                    'primary_metric': self.metrics_config['primary']
                }
            )
            print(f"ğŸ“Š wandb ì´ˆê¸°í™” ì™„ë£Œ - Project: {project_name}, Run: {run_name}")
            
        except Exception as e:
            print(f"âš ï¸ wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print("use_wandb=Falseë¡œ ì„¤ì •í•˜ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            self.use_wandb = False
        
    def _load_config(self, config_path):
        """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"ğŸ“ ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
        return config
    
    def evaluate_models(self, trained_models, X, y):
        """
        ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ í‰ê°€ (í‰ê°€ë§Œ ìˆ˜í–‰)
        
        Args:
            trained_models: {model_name: trained_model} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
            X: íŠ¹ì„± ë°ì´í„°  
            y: íƒ€ê¹ƒ ë°ì´í„°
        """
        print(f"ğŸ” {self.task.upper()} ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        print(f"íƒ€ê¹ƒ: {self.target}")
        print(f"ë°ì´í„° í¬ê¸°: {X.shape}")
        print(f"í‰ê°€í•  ëª¨ë¸ ìˆ˜: {len(trained_models)}")
        print("-" * 60)
        
        for model_name, model in trained_models.items():
            print(f"ğŸ“Š {model_name.upper()} ëª¨ë¸ í‰ê°€ ì¤‘...")
            
            try:
                # ì˜ˆì¸¡ (í•™ìŠµëœ ëª¨ë¸ë¡œ)
                y_pred = model.predict(X)
                
                # êµì°¨ê²€ì¦
                cv_scores = cross_val_score(
                    model, X, y, 
                    cv=self.cv_config['n_splits'],
                    scoring=self._get_cv_scoring()
                )
                
                # ê²°ê³¼ ì €ì¥
                self.trained_models[model_name] = model
                self.results[model_name] = {
                    'y_true': y,
                    'y_pred': y_pred,
                    'cv_scores': cv_scores,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'model': model
                }
                
                # íƒœìŠ¤í¬ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
                if self.task == 'regression':
                    self._calculate_regression_metrics(model_name, y, y_pred)
                else:
                    self._calculate_classification_metrics(model_name, y, y_pred, model, X)
                
                # wandb ë¡œê¹… (í˜ì´ì§€ ë°©ì‹ ë”°ë¼ ë‹¨ìˆœí™”)
                if self.use_wandb:
                    self._log_metrics_to_wandb(model_name)
                
                print(f"âœ… {model_name} í‰ê°€ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ {model_name} í‰ê°€ ì˜¤ë¥˜: {str(e)}")
                continue
        
        print("\nğŸ‰ ëª¨ë“  ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
        self._print_summary()
        
        # wandbì— ìµœì¢… ìš”ì•½ ë¡œê¹…
        if self.use_wandb:
            self._log_final_summary()
    
    def _log_metrics_to_wandb(self, model_name):
        """wandbì— ë©”íŠ¸ë¦­ ë¡œê¹… - í˜ì´ì§€ ë°©ì‹ì— ë”°ë¼ ë‹¨ìˆœí™”"""
        try:
            result = self.results[model_name]
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë¡œê¹…
            log_dict = {
                f"{model_name}_cv_mean": result['cv_mean'],
                f"{model_name}_cv_std": result['cv_std']
            }
            
            if self.task == 'regression':
                log_dict.update({
                    f"{model_name}_rmse": result['rmse'],
                    f"{model_name}_mae": result['mae'],
                    f"{model_name}_r2": result['r2']
                })
            else:
                log_dict.update({
                    f"{model_name}_accuracy": result['accuracy'],
                    f"{model_name}_precision": result['precision'],
                    f"{model_name}_recall": result['recall'], # ì¬í˜„ìœ¨. ì‹¤ì œ ì–‘ì„±ì¸ ê²ƒ ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨. ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
                    f"{model_name}_f1": result['f1'] # ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· . ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
                })
                
                # ROC AUC ìˆëŠ” ê²½ìš° ì¶”ê°€
                if 'roc_auc' in result:
                    log_dict[f"{model_name}_roc_auc"] = result['roc_auc']
            
            wandb.log(log_dict)
            
        except Exception as e:
            print(f"âš ï¸ wandb ë©”íŠ¸ë¦­ ë¡œê¹… ì˜¤ë¥˜: {str(e)}")
    
    def _log_final_summary(self):
        """ìµœì¢… ìš”ì•½ wandb ë¡œê¹…"""
        try:
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´
            best_model_info = self.get_best_model()
            if best_model_info:
                wandb.log({
                    "best_model_name": best_model_info['name'],
                    f"best_{self.metrics_config['primary']}": best_model_info['metrics'][self.metrics_config['primary']]
                })
            
            # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
            summary_df = self.get_summary_dataframe()
            if summary_df is not None:
                wandb.log({"model_comparison": wandb.Table(dataframe=summary_df)})
                
        except Exception as e:
            print(f"âš ï¸ wandb ìµœì¢… ìš”ì•½ ë¡œê¹… ì˜¤ë¥˜: {str(e)}")
    
    def _get_cv_scoring(self):
        """êµì°¨ê²€ì¦ ìŠ¤ì½”ì–´ë§ ë°©ë²•"""
        if self.task == 'regression':
            primary_metric = self.metrics_config['primary']
            if primary_metric == 'rmse':
                return 'neg_mean_squared_error'
            elif primary_metric == 'mae':
                return 'neg_mean_absolute_error'
            elif primary_metric == 'r2':
                return 'r2'
        else:
            return 'accuracy'
    
    def _calculate_regression_metrics(self, model_name, y_true, y_pred):
        """íšŒê·€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        self.results[model_name].update({
            'mse': mean_squared_error(y_true, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'mae': mean_absolute_error(y_true, y_pred),
            'r2': r2_score(y_true, y_pred)
        })
    
    def _calculate_classification_metrics(self, model_name, y_true, y_pred, model, X):
        """ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        self.results[model_name].update({
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted'),
            'recall': recall_score(y_true, y_pred, average='weighted'),
            'f1': f1_score(y_true, y_pred, average='weighted')
        })
        
        # ROC AUC (ì´ì§„ë¶„ë¥˜ì¸ ê²½ìš°)
        if len(np.unique(y_true)) == 2:
            if hasattr(model, 'predict_proba'):
                y_proba = model.predict_proba(X)[:, 1]
                self.results[model_name]['roc_auc'] = roc_auc_score(y_true, y_proba)
                self.results[model_name]['y_proba'] = y_proba
    
    def _print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½:")
        print("=" * 80)
        
        if self.task == 'regression':
            primary_metric = self.metrics_config['primary']
            print(f"{'ëª¨ë¸':^15} | {'RMSE':^10} | {'MAE':^10} | {'RÂ²':^10} | {'CV Score':^15}")
            print("-" * 80)
            
            for name, result in self.results.items():
                cv_score = result['cv_mean']
                if primary_metric == 'rmse':
                    cv_score = np.sqrt(-cv_score)  # neg_mseë¥¼ rmseë¡œ ë³€í™˜
                elif primary_metric == 'mae':
                    cv_score = -cv_score  # neg_maeë¥¼ maeë¡œ ë³€í™˜
                
                print(f"{name:^15} | {result['rmse']:^10.4f} | {result['mae']:^10.4f} | "
                      f"{result['r2']:^10.4f} | {cv_score:^8.4f}Â±{result['cv_std']:^.3f}")
        else:
            print(f"{'ëª¨ë¸':^15} | {'Accuracy':^10} | {'Precision':^10} | {'Recall':^10} | {'F1-Score':^10} | {'CV Score':^15}")
            print("-" * 95)
            
            for name, result in self.results.items():
                print(f"{name:^15} | {result['accuracy']:^10.4f} | {result['precision']:^10.4f} | "
                      f"{result['recall']:^10.4f} | {result['f1']:^10.4f} | "
                      f"{result['cv_mean']:^8.4f}Â±{result['cv_std']:^.3f}")
    
    def plot_results(self, figsize=(16, 12)):
        """ê²°ê³¼ ì‹œê°í™”"""
        if not self.results:
            print("âš ï¸ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        plt.style.use('seaborn-v0_8')
        
        if self.task == 'regression':
            self._plot_regression_results(figsize)
        else:
            self._plot_classification_results(figsize)
    
    def _plot_regression_results(self, figsize):
        """íšŒê·€ ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        fig.suptitle(f'ğŸ” íšŒê·€ ëª¨ë¸ í‰ê°€ ê²°ê³¼ (íƒ€ê¹ƒ: {self.target})', fontsize=16, fontweight='bold')
        
        models = list(self.results.keys())
        
        # 1. ì£¼ìš” ë©”íŠ¸ë¦­ ë¹„êµ
        metrics = ['rmse', 'mae', 'r2']
        colors = ['skyblue', 'lightcoral', 'lightgreen']
        
        for i, metric in enumerate(metrics):
            ax = axes[0, i]
            values = [self.results[model][metric] for model in models]
            
            bars = ax.bar(models, values, color=colors[i], alpha=0.7)
            ax.set_title(f'{metric.upper()} ë¹„êµ')
            ax.set_xlabel('ëª¨ë¸')
            ax.set_ylabel(metric.upper())
            plt.setp(ax.get_xticklabels(), rotation=45)
            
            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                       f'{value:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 2. êµì°¨ê²€ì¦ ì ìˆ˜ ë¶„í¬
        ax = axes[1, 0]
        cv_data = [self.results[model]['cv_scores'] for model in models]
        bp = ax.boxplot(cv_data, labels=models, patch_artist=True)
        
        # ë°•ìŠ¤í”Œë¡¯ ìƒ‰ìƒ ì„¤ì •
        colors_box = plt.cm.Set3(np.linspace(0, 1, len(models)))
        for patch, color in zip(bp['boxes'], colors_box):
            patch.set_facecolor(color)
        
        ax.set_title('êµì°¨ê²€ì¦ ì ìˆ˜ ë¶„í¬')
        ax.set_xlabel('ëª¨ë¸')
        ax.set_ylabel('CV Score')
        plt.setp(ax.get_xticklabels(), rotation=45)
        
        # 3. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
        primary_metric = self.metrics_config['primary']
        if primary_metric == 'r2':
            best_model = max(models, key=lambda x: self.results[x]['r2'])
        else:
            best_model = min(models, key=lambda x: self.results[x][primary_metric])
        
        ax = axes[1, 1]
        y_true = self.results[best_model]['y_true']
        y_pred = self.results[best_model]['y_pred']
        
        ax.scatter(y_true, y_pred, alpha=0.6, color='blue', s=20)
        ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
        ax.set_xlabel('ì‹¤ì œê°’')
        ax.set_ylabel('ì˜ˆì¸¡ê°’')
        ax.set_title(f'ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ({best_model})')
        
        # RÂ² í‘œì‹œ
        r2 = self.results[best_model]['r2']
        ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax.transAxes, 
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # 4. ì”ì°¨ í”Œë¡¯
        ax = axes[1, 2]
        residuals = y_true - y_pred
        ax.scatter(y_pred, residuals, alpha=0.6, color='green', s=20)
        ax.axhline(y=0, color='r', linestyle='--', linewidth=2)
        ax.set_xlabel('ì˜ˆì¸¡ê°’')
        ax.set_ylabel('ì”ì°¨')
        ax.set_title(f'ì”ì°¨ í”Œë¡¯ ({best_model})')
        
        plt.tight_layout()
        plt.show()
    
    def _plot_classification_results(self, figsize):
        """ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        fig.suptitle(f'ğŸ¯ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ê²°ê³¼ (íƒ€ê¹ƒ: {self.target})', fontsize=16, fontweight='bold')
        
        models = list(self.results.keys())
        
        # 1. ë©”íŠ¸ë¦­ ë¹„êµ
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
        
        ax = axes[0, 0]
        x = np.arange(len(models))
        width = 0.2
        
        for i, metric in enumerate(metrics):
            values = [self.results[model][metric] for model in models]
            bars = ax.bar(x + i*width, values, width, label=metric, color=colors[i], alpha=0.7)
            
            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.3f}', ha='center', va='bottom', fontsize=8)
        
        ax.set_xlabel('ëª¨ë¸')
        ax.set_ylabel('ì ìˆ˜')
        ax.set_title('ë¶„ë¥˜ ë©”íŠ¸ë¦­ ë¹„êµ')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(models, rotation=45)
        ax.legend()
        ax.set_ylim(0, 1.1)
        
        # 2. ROC ê³¡ì„  (ì´ì§„ë¶„ë¥˜)
        ax = axes[0, 1]
        has_roc = False
        for model_name in models:
            if 'roc_auc' in self.results[model_name]:
                has_roc = True
                y_true = self.results[model_name]['y_true']
                y_proba = self.results[model_name]['y_proba']
                fpr, tpr, _ = roc_curve(y_true, y_proba)
                auc = self.results[model_name]['roc_auc']
                
                ax.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')
        
        if has_roc:
            ax.plot([0, 1], [0, 1], 'k--', label='Random')
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title('ROC ê³¡ì„ ')
            ax.legend()
        else:
            ax.text(0.5, 0.5, 'ROC ê³¡ì„ \n(ì´ì§„ë¶„ë¥˜ ì „ìš©)', ha='center', va='center', transform=ax.transAxes)
        
        # 3. í˜¼ë™í–‰ë ¬ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
        best_model = max(models, key=lambda x: self.results[x]['accuracy'])
        ax = axes[0, 2]
        y_true = self.results[best_model]['y_true']
        y_pred = self.results[best_model]['y_pred']
        cm = confusion_matrix(y_true, y_pred)
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
        ax.set_title(f'í˜¼ë™í–‰ë ¬ ({best_model})')
        ax.set_xlabel('ì˜ˆì¸¡ê°’')
        ax.set_ylabel('ì‹¤ì œê°’')
        
        # 4. ëª¨ë¸ë³„ ì •í™•ë„
        ax = axes[1, 0]
        accuracies = [self.results[model]['accuracy'] for model in models]
        
        bars = ax.bar(models, accuracies, color='lightblue', alpha=0.7)
        ax.set_title('ëª¨ë¸ë³„ ì •í™•ë„')
        ax.set_xlabel('ëª¨ë¸')
        ax.set_ylabel('ì •í™•ë„')
        plt.setp(ax.get_xticklabels(), rotation=45)
        
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{acc:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 5. íŠ¹ì„± ì¤‘ìš”ë„ (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸)
        ax = axes[1, 1]
        tree_models = ['rf', 'lgbm', 'xgb', 'cat']
        available_tree_models = [m for m in tree_models if m in models]
        
        if available_tree_models:
            model_name = available_tree_models[0]
            model = self.results[model_name]['model']
            
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                feature_names = [f'Feature_{i}' for i in range(len(importances))]
                
                indices = np.argsort(importances)[::-1][:10]
                
                ax.barh(range(len(indices)), importances[indices], color='lightgreen', alpha=0.7)
                ax.set_title(f'íŠ¹ì„± ì¤‘ìš”ë„ ({model_name})')
                ax.set_xlabel('ì¤‘ìš”ë„')
                ax.set_yticks(range(len(indices)))
                ax.set_yticklabels([feature_names[i] for i in indices])
        else:
            ax.text(0.5, 0.5, 'íŠ¹ì„± ì¤‘ìš”ë„\n(íŠ¸ë¦¬ ëª¨ë¸ ì „ìš©)', ha='center', va='center', transform=ax.transAxes)
        
        # 6. êµì°¨ê²€ì¦ ì ìˆ˜
        ax = axes[1, 2]
        cv_data = [self.results[model]['cv_scores'] for model in models]
        bp = ax.boxplot(cv_data, labels=models, patch_artist=True)
        
        colors_box = plt.cm.Set3(np.linspace(0, 1, len(models)))
        for patch, color in zip(bp['boxes'], colors_box):
            patch.set_facecolor(color)
        
        ax.set_title('êµì°¨ê²€ì¦ ì ìˆ˜ ë¶„í¬')
        ax.set_xlabel('ëª¨ë¸')
        ax.set_ylabel('CV Score')
        plt.setp(ax.get_xticklabels(), rotation=45)
        
        plt.tight_layout()
        plt.show()
    
    def get_summary_dataframe(self):
        """ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜"""
        if not self.results:
            print("âš ï¸ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        summary_data = []
        for model_name, result in self.results.items():
            if self.task == 'regression': # íšŒê·€ ëª¨ë¸ë¸
                row = {
                    'ëª¨ë¸': model_name, # ëª¨ë¸ ì´ë¦„
                    'RMSE': result['rmse'], # ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ì œê³± í‰ê· ì— ë£¨íŠ¸ë¥¼ ì”Œìš´ ê°’. ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
                    'MAE': result['mae'], # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì ˆëŒ€ ì˜¤ì°¨ í‰ê· . ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
                    'RÂ²': result['r2'], # 0~1(ë˜ëŠ” ìŒìˆ˜ ê°€ëŠ¥) ë²”ìœ„ë¡œ í´ìˆ˜ë¡ ì¢‹ìŒ. 1ì´ë©´ ì™„ë²½í•œ ì„¤ëª….
                    'CV_Mean': result['cv_mean'], # êµì°¨ê²€ì¦ ì ìˆ˜ì˜ í‰ê· 
                    'CV_Std': result['cv_std'] # êµì°¨ê²€ì¦ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨
                }
            else:  # ë¶„ë¥˜ ëª¨ë¸
                row = {
                    'ëª¨ë¸': model_name,
                    'Accuracy': result['accuracy'], # ì •í™•ë„. ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ ì¼ì¹˜í•˜ëŠ” ë¹„ìœ¨. í´ìˆ˜ë¡ ì¢‹ìŒ.
                    'Precision': result['precision'], # ì •ë°€ë„. ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì–‘ì„±ì¸ ë¹„ìœ¨. ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
                    'Recall': result['recall'], # ì¬í˜„ìœ¨. ì‹¤ì œ ì–‘ì„±ì¸ ê²ƒ ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨. ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
                    'F1_Score': result['f1'], # ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· . ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
                    'CV_Mean': result['cv_mean'], # êµì°¨ê²€ì¦ ì ìˆ˜ì˜ í‰ê· 
                    'CV_Std': result['cv_std'] # êµì°¨ê²€ì¦ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨
                }
            summary_data.append(row)
        
        df = pd.DataFrame(summary_data)
        return df.sort_values(by=df.columns[1], ascending=(self.task == 'regression'))
    
    def get_best_model(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°˜í™˜"""
        if not self.results:
            return None
        
        if self.task == 'regression':
            primary_metric = self.metrics_config['primary']
            if primary_metric == 'r2':
                best_name = max(self.results.keys(), key=lambda x: self.results[x]['r2'])
            else:
                best_name = min(self.results.keys(), key=lambda x: self.results[x][primary_metric])
        else:
            best_name = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])
        
        return {
            'name': best_name,
            'model': self.trained_models[best_name],
            'metrics': self.results[best_name]
        }
    
    def finish_wandb(self):
        """wandb ì‹¤í–‰ ì¢…ë£Œ"""
        if self.use_wandb:
            wandb.finish()
            print("ğŸ“Š wandb ì‹¤í–‰ ì¢…ë£Œë¨")


if __name__ == "__main__":
    print("ëª¨ë¸ í‰ê°€ ë„êµ¬")
    print("=" * 30)
    print("ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ í‰ê°€í•©ë‹ˆë‹¤.")
    print()
    print("ì‚¬ìš©ë²•:")
    print("1. evaluator = ModelEvaluator('config.yaml', use_wandb=True)")
    print("2. evaluator.evaluate_models(pretrained_models, X_test, y_test)")
    print("3. evaluator.plot_results()")
    print("4. evaluator.finish_wandb()")