import os
import sys
sys.path.append('/app')

import pandas as pd
import numpy as np
import pickle
from io import BytesIO
from datetime import datetime
import pytz
from src.utils.utils import get_s3_client


def get_latest_parquet_from_s3(bucket: str = None):
    """S3ì—ì„œ ìµœì‹  parquet íŒŒì¼ ìë™ ë¡œë“œ"""
    if bucket is None:
        bucket = os.getenv('S3_BUCKET')
    
    s3_client = get_s3_client()
    
    # ml_dataset/ ê²½ë¡œì—ì„œ ìµœì‹  ë‚ ì§œ í´ë” íƒìƒ‰
    prefix = "ml_dataset/"
    
    # ëª¨ë“  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
    
    # parquet íŒŒì¼ë§Œ í•„í„°ë§í•˜ê³  ìµœì‹ ìˆœ ì •ë ¬
    parquet_files = []
    for obj in response.get('Contents', []):
        key = obj['Key']
        if key.endswith('.parquet'):
            parquet_files.append((obj['LastModified'], key))
    
    if not parquet_files:
        raise FileNotFoundError("S3ì— parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # ìµœì‹  íŒŒì¼ ì„ íƒ
    latest_file = sorted(parquet_files, reverse=True)[0][1]
    print(f"ğŸ“‚ ìµœì‹  parquet íŒŒì¼: {latest_file}")
    
    # S3ì—ì„œ parquet ì½ê¸°
    obj = s3_client.get_object(Bucket=bucket, Key=latest_file)
    df = pd.read_parquet(BytesIO(obj['Body'].read()))
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    return df


def load_preprocessor_and_model_from_s3(experiment_name: str, bucket: str = None):
    """
    S3ì—ì„œ ì „ì²˜ë¦¬ ê°ì²´ì™€ ëª¨ë¸ì„ ë¡œë“œ
    
    í•µì‹¬: preprocessor.pklì— ì „ì²˜ë¦¬ ë¡œì§ì´ ëª¨ë‘ ì €ì¥ë˜ì–´ ìˆìŒ!
    """
    if bucket is None:
        bucket = os.getenv('S3_BUCKET')
    
    s3_client = get_s3_client()
    
    # Preprocessor ë¡œë“œ (ì „ì²˜ë¦¬ ë¡œì§ í¬í•¨!)
    try:
        preprocessor_key = f"models/{experiment_name}/model_artifact/preprocessor.pkl"
        preprocessor_obj = s3_client.get_object(Bucket=bucket, Key=preprocessor_key)
        preprocessor = pickle.load(BytesIO(preprocessor_obj['Body'].read()))
        print(f"âœ… Preprocessor ë¡œë“œ ì™„ë£Œ (fitëœ ì „ì²˜ë¦¬ ê°ì²´)")
    except Exception as e:
        print(f"âš ï¸  Preprocessor ì—†ìŒ, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© í•„ìš”")
        raise FileNotFoundError(f"Preprocessor not found: {e}")
    
    # ëª¨ë¸ ë¡œë“œ
    model_key = f"models/{experiment_name}/model_artifact/model.pkl"
    model_obj = s3_client.get_object(Bucket=bucket, Key=model_key)
    model = pickle.load(BytesIO(model_obj['Body'].read()))
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    return preprocessor, model


def autobatch_predict(experiment_name: str = None, output_path: str = None):
    """
    ì™„ì „ ìë™í™”ëœ ë°°ì¹˜ ì¶”ë¡ 
    
    íŠ¹ì§•:
    - ì „ì²˜ë¦¬ ë¡œì§ í•˜ë“œì½”ë”© ì—†ìŒ âœ…
    - S3ì— ì €ì¥ëœ preprocessor ê°ì²´ ì‚¬ìš© âœ…
    - DAGì—ì„œ ì „ì²˜ë¦¬ ë³€ê²½ ì‹œ ìë™ ì ìš© âœ…
    
    Args:
        experiment_name: ëª¨ë¸ëª… (ê¸°ë³¸: 'weather-predictor-018')
        output_path: ê²°ê³¼ CSV ì €ì¥ ê²½ë¡œ (ì˜µì…˜)
    
    Returns:
        result_df: ì˜ˆì¸¡ ê²°ê³¼ DataFrame
    """
    if experiment_name is None:
        experiment_name = os.getenv('DEFAULT_MODEL_NAME', 'weather-predictor-018')
    
    print(f"ğŸš€ ìë™í™” ë°°ì¹˜ ì¶”ë¡  ì‹œì‘: {experiment_name}")
    print(f"ğŸ’¡ íŠ¹ì§•: ì „ì²˜ë¦¬ ë¡œì§ í•˜ë“œì½”ë”© ì—†ìŒ, ì™„ì „ ìë™í™”!")
    
    # 1. S3ì—ì„œ ìµœì‹  ë°ì´í„° ë¡œë“œ
    df = get_latest_parquet_from_s3()
    
    # datetimeì„ ì„œìš¸ ì‹œê°„ëŒ€(KST)ë¡œ ë³€í™˜
    kst = pytz.timezone('Asia/Seoul')
    if 'datetime' in df.columns:
        if df['datetime'].dt.tz is None:
            df['datetime'] = pd.to_datetime(df['datetime']).dt.tz_localize('UTC').dt.tz_convert(kst)
        else:
            df['datetime'] = df['datetime'].dt.tz_convert(kst)
        print(f"ğŸ• ì‹œê°„ëŒ€ ë³€í™˜: UTC â†’ KST (ì„œìš¸) | ìƒ˜í”Œ: {df['datetime'].iloc[0]}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥ (ì˜ˆì¸¡ í›„ ë³µì›ìš©)
    metadata_cols = ['datetime', 'station_id', 'region']
    metadata = df[metadata_cols].copy() if all(col in df.columns for col in metadata_cols) else None
    
    # ê¸°ìƒ ë°ì´í„° ì €ì¥ (ê²°ê³¼ì— í¬í•¨)
    weather_cols = ['temperature', 'humidity', 'rainfall', 'pm10', 'wind_speed', 'pressure', 'region']
    weather_data = df[[col for col in weather_cols if col in df.columns]].copy()
    
    # 2. Preprocessor + Model ë¡œë“œ
    preprocessor, model = load_preprocessor_and_model_from_s3(experiment_name)
    
    # 3. ì „ì²˜ë¦¬ ìë™ ì ìš©! (í•˜ë“œì½”ë”© ì—†ìŒ!)
    print(f"ğŸ”§ ì „ì²˜ë¦¬ ìë™ ì ìš© (preprocessor.transform)")
    X_processed = preprocessor.transform(df)
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {X_processed.shape}")
    
    # 4. ì˜ˆì¸¡
    print(f"ğŸ¤– ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...")
    predictions = model.predict(X_processed)
    print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ")
    
    # 5. ê²°ê³¼ ìƒì„±
    result_df = pd.DataFrame()
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    if metadata is not None:
        for col in metadata_cols:
            if col in metadata.columns:
                result_df[col] = metadata[col].values
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    result_df['predicted_comfort_score'] = predictions
    
    # ê¸°ìƒ ë°ì´í„° ì¶”ê°€
    for col in weather_data.columns:
        result_df[col] = weather_data[col].values
    
    # 6. ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
    if output_path:
        result_df.to_csv(output_path, index=False)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    print(f"ğŸ‰ ìë™í™” ë°°ì¹˜ ì¶”ë¡  ì™„ë£Œ!")
    print(f"ğŸ¯ ì˜ˆì¸¡ëœ ì¾Œì ì§€ìˆ˜: {predictions[0]:.1f}/100")
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì˜ˆì¸¡ í†µê³„:")
    print(f"   í‰ê· : {predictions.mean():.1f}")
    print(f"   ìµœì†Œ: {predictions.min():.1f}")
    print(f"   ìµœëŒ€: {predictions.max():.1f}")
    
    return result_df


if __name__ == "__main__":
    import fire
    
    try:
        fire.Fire(autobatch_predict)
    except TypeError:
        # fire ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¶œë ¥ ì—ëŸ¬ ë¬´ì‹œ
        pass 