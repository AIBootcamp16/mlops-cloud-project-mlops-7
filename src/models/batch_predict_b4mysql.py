import os
import sys
sys.path.append('/app')

import pandas as pd
import numpy as np
import pickle
import json
from io import BytesIO
from datetime import datetime
from src.utils.utils import get_s3_client

def get_latest_parquet_from_s3(bucket: str = None):
    """S3ì—ì„œ ìµœì‹  parquet íŒŒì¼ ìë™ ë¡œë“œ"""
    if bucket is None:
        bucket = os.getenv('S3_BUCKET')
    
    s3_client = get_s3_client()
    
    # ml_dataset/ ê²½ë¡œì—ì„œ ìµœì‹  ë‚ ì§œ í´ë” íƒìƒ‰
    prefix = "ml_dataset/"
    
    # ëª¨ë“  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
    
    # parquet íŒŒì¼ë§Œ í•„í„°ë§í•˜ê³  ìµœì‹ ìˆœ ì •ë ¬
    parquet_files = []
    for obj in response.get('Contents', []):
        key = obj['Key']
        if key.endswith('.parquet'):
            parquet_files.append((obj['LastModified'], key))
    
    if not parquet_files:
        raise FileNotFoundError("S3ì— parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # ìµœì‹  íŒŒì¼ ì„ íƒ
    latest_file = sorted(parquet_files, reverse=True)[0][1]
    print(f"ğŸ“‚ ìµœì‹  parquet íŒŒì¼: {latest_file}")
    
    # S3ì—ì„œ parquet ì½ê¸°
    obj = s3_client.get_object(Bucket=bucket, Key=latest_file)
    df = pd.read_parquet(BytesIO(obj['Body'].read()))
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    return df

def load_model_from_s3(experiment_name: str, bucket: str = None):
    """S3ì—ì„œ ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, config, feature_columns ë¡œë“œ"""
    if bucket is None:
        bucket = os.getenv('S3_BUCKET')
    
    s3_client = get_s3_client()
    
    # ëª¨ë¸ ë¡œë“œ
    model_key = f"models/{experiment_name}/model_artifact/model.pkl"
    model_obj = s3_client.get_object(Bucket=bucket, Key=model_key)
    model = pickle.load(BytesIO(model_obj['Body'].read()))
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_key = f"models/{experiment_name}/model_artifact/scaler.pkl"
    scaler_obj = s3_client.get_object(Bucket=bucket, Key=scaler_key)
    scaler = pickle.load(BytesIO(scaler_obj['Body'].read()))
    
    # config ë¡œë“œ
    config_key = f"models/{experiment_name}/config/train_config.json"
    config_obj = s3_client.get_object(Bucket=bucket, Key=config_key)
    config = json.load(config_obj['Body'])
    
    # feature_columns ë¡œë“œ
    feature_col_key = f"models/{experiment_name}/config/feature_columns.json"
    feature_col_obj = s3_client.get_object(Bucket=bucket, Key=feature_col_key)
    feature_columns = json.load(feature_col_obj['Body'])
    
    return model, scaler, config, feature_columns

def preprocess_for_prediction(df, feature_columns):
    """split.pyì™€ ë™ì¼í•œ í›„ì²˜ë¦¬ ë¡œì§ + ì»¬ëŸ¼ ë§ì¶”ê¸°"""
    print("ğŸ”§ ì „ì²˜ë¦¬ ì‹œì‘")
    
    # ì¹´í…Œê³ ë¦¬ ì´ë¦„ í†µì¼
    if 'pm10_grade' in df.columns:
        df['pm10_grade'] = df['pm10_grade'].replace({
            'unhealthy': 'bad',
            'very_unhealthy': 'very_bad'
        })

    # íƒ€ê²Ÿ ì œì™¸
    target_col = "comfort_score"
    exclude_cols = [target_col, "pm10", "datetime", "station_id"]
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    X = df[feature_cols].copy()
    X = X.replace([-99, -9], np.nan)
    
    # ê³ ê²°ì¸¡ ì»¬ëŸ¼ ì œê±° (50% ì´ìƒ)
    high_missing_cols = [col for col in X.columns if X[col].isnull().sum() / len(X) > 0.5]
    if high_missing_cols:
        print(f"ê³ ê²°ì¸¡ ì»¬ëŸ¼ ì œê±°: {high_missing_cols}")
        X = X.drop(columns=high_missing_cols)
    
    # ê²°ì¸¡ì¹˜ í‰ê·  ëŒ€ì²´
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].mean())
    
    # ì›í•«ì¸ì½”ë”©
    categorical_cols = ['season', 'temp_category', 'pm10_grade', 'region']
    categorical_cols = [col for col in categorical_cols if col in X.columns]
    
    if categorical_cols:
        print(f"ì›í•«ì¸ì½”ë”©: {categorical_cols}")
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
    
    # í•™ìŠµ ì‹œ ì»¬ëŸ¼ì— ë§ì¶° ë³´ì •
    for col in feature_columns:
        if col not in X.columns:
            X[col] = 0
    X = X[feature_columns]  # ìˆœì„œê¹Œì§€ ë™ì¼í•˜ê²Œ
    
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {X.shape}")
    return X

def batch_predict(experiment_name: str = None, output_path: str = None):
    """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰ (S3 ìµœì‹  ë°ì´í„° ìë™ ë¡œë“œ)"""
    if experiment_name is None:
        experiment_name = os.getenv('DEFAULT_MODEL_NAME', 'weather-predictor-018')
    
    print(f"ğŸ”„ ë°°ì¹˜ ì¶”ë¡  ì‹œì‘: {experiment_name}")
    
    # 1. S3ì—ì„œ ìµœì‹  parquet íŒŒì¼ ë¡œë“œ (ì´ë¯¸ ì „ì²˜ë¦¬ë¨)
    df = get_latest_parquet_from_s3()
    
    # 2. ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, config, feature_columns ë¡œë“œ
    model, scaler, config, feature_columns = load_model_from_s3(experiment_name)
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (í”¼ì²˜: {len(feature_columns)}ê°œ)")
    
    # 3. ì „ì²˜ë¦¬ (split.py ë¡œì§ + ì»¬ëŸ¼ ë§ì¶”ê¸°)
    X = preprocess_for_prediction(df, feature_columns)
    X_scaled = scaler.transform(X)
    
    # 4. ì˜ˆì¸¡
    predictions = model.predict(X_scaled)
    
    # 5. ê²°ê³¼ ì €ì¥
    result_df = df[['datetime', 'station_id']].copy()
    result_df['predicted_comfort_score'] = predictions
    
    if output_path:
        result_df.to_csv(output_path, index=False)
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_path}")
    
    print(f"ğŸ‰ ë°°ì¹˜ ì¶”ë¡  ì™„ë£Œ: {len(predictions)}ê°œ ì˜ˆì¸¡")
    print(f"ğŸ¯ ì˜ˆì¸¡ëœ ì¾Œì ì§€ìˆ˜: {predictions[0]:.1f}/100")
    
    return result_df

if __name__ == "__main__":
    import fire
    
    try:
        fire.Fire(batch_predict)
    except TypeError:
        # fire ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¶œë ¥ ì—ëŸ¬ ë¬´ì‹œ
        pass